{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59856495-7b8a-4717-80d4-4f0b9bd6fb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/ikulkar1/miniconda3/envs/203A3E1/lib/python3.13/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ikulkar1/miniconda3/envs/203A3E1/lib/python3.13/site-packages (from tensorboard) (2.2.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/ikulkar1/miniconda3/envs/203A3E1/lib/python3.13/site-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ikulkar1/miniconda3/envs/203A3E1/lib/python3.13/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ikulkar1/miniconda3/envs/203A3E1/lib/python3.13/site-packages (from tensorboard) (2.2.3)\n",
      "Requirement already satisfied: packaging in /home/ikulkar1/miniconda3/envs/203A3E1/lib/python3.13/site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/ikulkar1/miniconda3/envs/203A3E1/lib/python3.13/site-packages (from tensorboard) (6.30.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ikulkar1/miniconda3/envs/203A3E1/lib/python3.13/site-packages (from tensorboard) (75.8.1)\n",
      "Requirement already satisfied: six>1.9 in /home/ikulkar1/miniconda3/envs/203A3E1/lib/python3.13/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ikulkar1/miniconda3/envs/203A3E1/lib/python3.13/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ikulkar1/miniconda3/envs/203A3E1/lib/python3.13/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ikulkar1/miniconda3/envs/203A3E1/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb80d8f-09ae-4f17-b4da-7ceeba517481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from conlleval import evaluate as conllevaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b935af2b-5413-44a0-8d98-8d335e44bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c9563d4-c5d3-4856-88f5-41fa92424978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f86b9f-21d5-4041-9e51-c3e25e8b893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51c561-b1f8-4722-9fa9-6236ed9c3c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, char_embedding_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.char_embed = nn.Embedding(10, char_embedding_dim)\n",
    "\n",
    "        \n",
    "        self.char_cnn = nn.Conv2d(in_channels=1, out_channels=char_embedding_dim, kernel_size=(1, char_embedding_dim))\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        \n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        forward_var = init_alphas\n",
    "        device_info = forward_var.device \n",
    "        for feat in feats:\n",
    "            alphas_t = []  \n",
    "            for next_tag in range(self.tagset_size):\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size).to(device_info)\n",
    "                trans_score = self.transitions[next_tag].view(1, -1).to(device_info)\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "         \n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]].to(device_info)\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence): \n",
    "        self.hidden = self.init_hidden()\n",
    "        sentence = sentence.to(self.word_embeds.weight.device)\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        self.hidden = tuple(h.to(embeds.device) for h in self.hidden)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden) \n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "\n",
    "    def get_char_indices(self, word_idx):\n",
    "        \"\"\"\n",
    "        Extracts character indices using nltk.word_tokenize.\n",
    "        \"\"\"\n",
    "        char_idx = [word_to_ix[char] for char in train_data[word_idx]['tokens']] \n",
    "        return char_idx\n",
    "\n",
    "    def _get_lstm_features_cnn(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        sentence = sentence.to(self.word_embeds.weight.device)\n",
    "        char_embeddings, char_ids = self.char_embed, []\n",
    "        for word_idx in sentence:\n",
    "           \n",
    "            chars = self.get_char_indices(word_idx) \n",
    "            char_ids.append(torch.tensor(chars).to(device))\n",
    "                            \n",
    "        char_ids = pad_sequence(char_ids, batch_first=True, padding_value=0) \n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=char_embeddings.num_embeddings,  out_channels=11,  kernel_size=3, padding=1)  \n",
    "        cnn_out = self.conv1(char_embeddings(char_ids))\n",
    "        lstm_out = torch.max(F.relu(cnn_out), dim=2)[0] \n",
    "        lstm_out = lstm_out.view(len(sentence), -1)\n",
    "        lstm_out, self.hidden = self.lstm(lstm_out, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim * 2)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        score = torch.zeros(1, device=feats.device)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.,device=feats.device)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = [] \n",
    "            viterbivars_t = []  \n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  \n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc68d58-11b3-48b0-ac17-f698b66bddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_point(sent):\n",
    "    \"\"\"\n",
    "        Creates a dictionary from String to an Array of Strings representing the data.  \n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    sent = [s.strip().split() for s in sent]\n",
    "    dic['tokens'] = ['<START>'] + [s[0] for s in sent] + ['<STOP>']\n",
    "    dic['pos'] = ['<START>'] + [s[1] for s in sent] + ['<STOP>']\n",
    "    dic['NP_chunk'] = ['<START>'] + [s[2] for s in sent] + ['<STOP>']\n",
    "    dic['gold_tags'] = ['<START>'] + [s[3] for s in sent] + ['<STOP>']\n",
    "    return dic\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Reads the CoNLL 2003 data into an array of dictionaries (a dictionary for each data point).\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        sent = []\n",
    "        for line in f.readlines():\n",
    "            if line.strip():\n",
    "                sent.append(line)\n",
    "            else:\n",
    "                data.append(make_data_point(sent))\n",
    "                sent = []\n",
    "        data.append(make_data_point(sent))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7335387-90c4-4d86-b994-a38f074004aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['<START>', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.', '<STOP>'], 'pos': ['<START>', 'NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NN', '.', '<STOP>'], 'NP_chunk': ['<START>', 'I-NP', 'I-VP', 'I-NP', 'I-NP', 'I-VP', 'I-VP', 'I-NP', 'I-NP', 'O', '<STOP>'], 'gold_tags': ['<START>', 'I-ORG', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O', '<STOP>']}\n"
     ]
    }
   ],
   "source": [
    "train_data = read_data('ner.train')\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b8d59d5-d46a-4661-84ef-e9b17adba90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['<START>', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.', '<STOP>'], 'pos': ['<START>', 'NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NN', '.', '<STOP>'], 'NP_chunk': ['<START>', 'I-NP', 'I-VP', 'I-NP', 'I-NP', 'I-VP', 'I-VP', 'I-NP', 'I-NP', 'O', '<STOP>'], 'gold_tags': ['<START>', 'I-ORG', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O', '<STOP>']}\n",
      "{'tokens': ['<START>', 'CRICKET', '-', 'LEICESTERSHIRE', 'TAKE', 'OVER', 'AT', 'TOP', 'AFTER', 'INNINGS', 'VICTORY', '.', '<STOP>'], 'pos': ['<START>', 'NNP', ':', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', '.', '<STOP>'], 'NP_chunk': ['<START>', 'I-NP', 'O', 'I-NP', 'I-NP', 'I-PP', 'I-NP', 'I-NP', 'I-NP', 'I-NP', 'I-NP', 'O', '<STOP>'], 'gold_tags': ['<START>', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '<STOP>']}\n",
      "{'tokens': ['<START>', 'SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.', '<STOP>'], 'pos': ['<START>', 'NN', ':', 'NNP', 'VB', 'NNP', 'NNP', ',', 'NNP', 'IN', 'DT', 'NN', '.', '<STOP>'], 'NP_chunk': ['<START>', 'I-NP', 'O', 'I-NP', 'I-VP', 'I-NP', 'I-NP', 'O', 'I-NP', 'I-PP', 'I-NP', 'I-NP', 'O', '<STOP>'], 'gold_tags': ['<START>', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', '<STOP>']}\n"
     ]
    }
   ],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 40\n",
    "HIDDEN_DIM = 40\n",
    "CHAR_EMBEDDING_DIM = 4\n",
    "\n",
    "dev_data = read_data('ner.dev')\n",
    "test_data = read_data('ner.test')\n",
    "train_data = read_data('ner.train')\n",
    "print(train_data[0])\n",
    "print(dev_data[0])\n",
    "print(test_data[0])\n",
    "\n",
    "word_2_idx = {}\n",
    "for sentence in train_data + dev_data + test_data:\n",
    "    for word in sentence['tokens']:\n",
    "        if word not in word_2_idx:\n",
    "            word_2_idx[word] = len(word_2_idx)\n",
    "\n",
    "tag_2_idx = {}\n",
    "for sentence in train_data + dev_data + test_data:\n",
    "    for word in sentence['gold_tags']:\n",
    "        if word not in tag_2_idx:\n",
    "            tag_2_idx[word] = len(tag_2_idx)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dc25b31-0bf5-462c-aba1-acfd536d604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word_2_idx), tag_2_idx, EMBEDDING_DIM, HIDDEN_DIM, CHAR_EMBEDDING_DIM)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9904547-2a37-4609-9073-2be6595a5cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(22.7877, device='cuda:0'), [1, 9, 9, 1, 9, 1, 9, 1, 9, 1, 9])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(train_data[0]['tokens'], word_2_idx)\n",
    "    precheck_tags = torch.tensor([tag_2_idx[t] for t in train_data[0]['gold_tags']], dtype=torch.long).to(device)\n",
    "    print(model(precheck_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b86081c8-57b9-4e19-9adf-dae104d3cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minibatches(training_data, batch_size):\n",
    "    minibatches = []\n",
    "    for i in range(0, len(training_data), batch_size):\n",
    "        minibatch = training_data[i:i + batch_size]\n",
    "        minibatches.append(minibatch)\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92ce7fc9-3991-4134-95d0-0053c7db20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbf3a956-b3e4-476d-bc8c-5a36c36d2660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [39:54, 20.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for one epoch with batch size 128: 2394.23 seconds\n",
      "Epochs 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [40:02, 20.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [39:45, 20.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [39:41, 20.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [39:41, 20.18s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "start_time = time.time()\n",
    "for epoch in range(5):  \n",
    "    print(f\"Epochs {epoch}\")\n",
    "    minibatches = generate_minibatches(train_data, batch_size)\n",
    "    for i, minibatch in tqdm(enumerate(minibatches)):\n",
    "        model.zero_grad()\n",
    "\n",
    "        sentences_in = [prepare_sequence(sentence['tokens'], word_2_idx) for sentence in minibatch]\n",
    "        sentences_in = pad_sequence(sentences_in, batch_first=True)\n",
    "        targets = [torch.tensor([tag_2_idx[t] for t in sentence['gold_tags']], dtype=torch.long) for sentence in minibatch]\n",
    "        targets = pad_sequence(targets, batch_first=True)\n",
    "\n",
    "        loss = 0\n",
    "        for sentence_in, target in zip(sentences_in, targets):\n",
    "            loss += model.neg_log_likelihood(sentence_in, target)\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch * len(minibatches) + i)\n",
    "\n",
    "     \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if epoch < 1:\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Training time for one epoch with batch size {batch_size}: {elapsed_time:.2f} seconds\")\n",
    "    torch.save(model.state_dict(), f'bilstm_crf_model_epoch_{epoch}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31598398-b3dd-4c28-a86a-f007c69c1f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', 'I-ORG', 'O', 'I-MISC', '<STOP>', 'I-PER', 'I-LOC', 'B-LOC', 'B-MISC', 'B-ORG']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3466it [00:54, 63.79it/s] \n"
     ]
    }
   ],
   "source": [
    "dev_seqs = [prepare_sequence(example['tokens'], word_2_idx) for example in dev_data]\n",
    "test_seqs = [prepare_sequence(example['tokens'], word_2_idx) for example in test_data]\n",
    "\n",
    "\n",
    "loaded_model = BiLSTM_CRF(len(word_2_idx), tag_2_idx, EMBEDDING_DIM, HIDDEN_DIM, CHAR_EMBEDDING_DIM)\n",
    "loaded_model.load_state_dict(torch.load('bilstm_crf_model_epoch_4.pth'))\n",
    "loaded_model.to(device)\n",
    "\n",
    "tag_key_list = list(tag_2_idx.keys())\n",
    "print(tag_key_list)\n",
    "\n",
    "with open('dev_predictions.txt', 'w') as f_dev, open('test_predictions.txt', 'w') as f_test:\n",
    "  with torch.no_grad():\n",
    "    for dev_sent, test_sent in tqdm(zip(dev_seqs, test_seqs)):\n",
    "    \n",
    "      dev_sent = dev_sent.to(device)\n",
    "      test_sent = test_sent.to(device)\n",
    "\n",
    "      _ , dev_predicted_tags = loaded_model(dev_sent)\n",
    "      _, test_predicted_tags = loaded_model(test_sent)\n",
    "\n",
    "      dev_tags = [tag_key_list[tag_id] for tag_id in dev_predicted_tags]\n",
    "      test_tags = [tag_key_list[tag_id] for tag_id in test_predicted_tags]\n",
    "\n",
    "      f_dev.write(' '.join(dev_tags) + '\\n')\n",
    "      f_test.write(' '.join(test_tags) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f04fe-e3b1-4641-9831-b9828aa191ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "sentence = []\n",
    "with open('dev_predictions.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        data.append(line.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b00788f1-1745-4de4-b7d1-dc4aeebe2db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ce66cf0-1474-427c-b4c5-0e195558cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = []\n",
    "for example in dev_data:\n",
    "    gold.extend(example['gold_tags'][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5608d0b3-ed04-49b1-b671-e02405b9a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = []\n",
    "for i in data:\n",
    "    val_data.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5631b104-c448-42b5-a4ae-e1a019c897a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 1058 phrases; correct: 23.\n",
      "accuracy:   0.56%; (non-O)\n",
      "accuracy:  81.72%; precision:   2.17%; recall:   0.39%; FB1:   0.66\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  13\n",
      "             MISC: precision:   2.31%; recall:   0.88%; FB1:   1.27  346\n",
      "              ORG: precision:   0.00%; recall:   0.00%; FB1:   0.00  13\n",
      "              PER: precision:   2.19%; recall:   0.82%; FB1:   1.19  686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.1739130434782608, 0.38871049518337, 0.6594982078853047)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conllevaluate(gold, val_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba34f650-e5f7-42e1-8b4f-364bf3903231",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "sentence = []\n",
    "with open('test_predictions.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        data.append(line.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96379ea1-15e1-4aee-9f7e-b27b6d9aa110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "837e7f45-e90b-4edf-85d3-63f343497da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = []\n",
    "for example in dev_data:\n",
    "    gold.extend(example['gold_tags'][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87db4d08-b9ca-41a1-9e01-36a261df1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = []\n",
    "for i in data:\n",
    "    val_data.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef928d66-1b2d-43ce-baed-45423b32f368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 898 phrases; correct: 15.\n",
      "accuracy:   0.49%; (non-O)\n",
      "accuracy:  81.97%; precision:   1.67%; recall:   0.25%; FB1:   0.44\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  7\n",
      "             MISC: precision:   1.58%; recall:   0.55%; FB1:   0.81  316\n",
      "              ORG: precision:   0.00%; recall:   0.00%; FB1:   0.00  7\n",
      "              PER: precision:   1.76%; recall:   0.55%; FB1:   0.83  568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.670378619153675, 0.2535068446848065, 0.4402054292002935)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conllevaluate(gold, val_data)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "203A3E1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
