{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHdbUbDpdTBp"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from conlleval import evaluate as conllevaluate\n",
    "from tqdm import tqdm\n",
    "import math \n",
    "\n",
    "directory = 'adagrad_results/'\n",
    "\n",
    "def decode(input_length, tagset, score, debug=False):\n",
    "    \"\"\"\n",
    "    Compute the highest scoring sequence according to the scoring function.\n",
    "    \"\"\"\n",
    "    viterbi = [[0 for _ in range(input_length)] for _ in range(len(tagset))]\n",
    "    backpointer = [[0 for _ in range(input_length)] for _ in range(len(tagset))]\n",
    "    best_path = []\n",
    "\n",
    "    for i, tag in enumerate(tagset):\n",
    "        viterbi[i][1] = score(tag, \"<START>\", 1)\n",
    "    \n",
    "    if debug: print(viterbi)\n",
    "        \n",
    "    for t in range(2, input_length - 1):\n",
    "        for s, tag in enumerate(tagset):\n",
    "            max_val = 0\n",
    "            max_index = 0\n",
    "            for b, prev_tag in enumerate(tagset):\n",
    "                curr_val = viterbi[b][t - 1] + score(tag, prev_tag, t)\n",
    "                if curr_val > max_val:\n",
    "                    max_val = curr_val\n",
    "                    max_index = b\n",
    "            viterbi[s][t] = max_val\n",
    "            backpointer[s][t] = max_index\n",
    "\n",
    "    for i, tag in enumerate(tagset):\n",
    "        viterbi[i][input_length - 1] = viterbi[i][input_length - 2] + score(\"<STOP>\", tag, input_length - 1)\n",
    "\n",
    "    best_path_prob = 0\n",
    "    index_to_best_path = 0\n",
    "    for i in range(len(tagset)):\n",
    "        if viterbi[i][-1] > best_path_prob:\n",
    "            best_path_prob = viterbi[i][-1]\n",
    "            index_to_best_path = i\n",
    "\n",
    "    best_path = [\"<STOP>\"]\n",
    "    for i in range(input_length - 1, 0, -1):\n",
    "        if i == 1:\n",
    "            best_path.insert(0, \"<START>\")\n",
    "        else:\n",
    "            index_to_best_path = backpointer[index_to_best_path][i]\n",
    "            best_path.insert(0, tagset[index_to_best_path])\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(tag_seq, input_length, score):\n",
    "    \"\"\"\n",
    "    Computes the total score of a tag sequence \n",
    "    \"\"\"\n",
    "    total_score = 0\n",
    "    for i in range(1, input_length):\n",
    "        total_score += score(tag_seq[i], tag_seq[i - 1], i)\n",
    "    return total_score\n",
    "\n",
    "\n",
    "def compute_features(tag_seq, input_length, features):\n",
    "    \"\"\"\n",
    "    Compute f(xi, yi)\n",
    "    \"\"\"\n",
    "    feats = FeatureVector({})\n",
    "    for i in range(1, input_length):\n",
    "        feats.times_plus_equal(1, features.compute_features(tag_seq[i], tag_seq[i - 1], i))\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.path.exists('ner.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adagrad(training_size, epochs, gradient, parameters, training_observer, alpha=1.0, epsilon=1e-8, debug=False):\n",
    "    parameters_list, temp  = [], FeatureVector({})\n",
    "    data_indices = [i for i in range(training_size)]\n",
    "    random.shuffle(data_indices)\n",
    "    for i in range(epochs):\n",
    "        print(f'EPOCH {i}')\n",
    "        for t in tqdm(data_indices):\n",
    "            if debug: print(t)\n",
    "            temp.power_plus(2, gradient(t))\n",
    "            parameters.times_plus_equal(-1, gradient(t).sqrt_div(temp, 1e-8))\n",
    "        print(training_observer(i, parameters))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(data, feature_names, tagset, epochs):\n",
    "    \"\"\"\n",
    "    Trains the model on the data and returns the parameters\n",
    "    \"\"\"\n",
    "    parameters = FeatureVector({})  \n",
    "\n",
    "    def perceptron_gradient(i):\n",
    "        inputs = data[i]\n",
    "        input_len = len(inputs['tokens'])\n",
    "        gold_labels = inputs['gold_tags']\n",
    "        features = Features(inputs, feature_names)\n",
    "\n",
    "        def score(cur_tag, pre_tag, i):\n",
    "            return parameters.dot_product(features.compute_features(cur_tag, pre_tag, i))\n",
    "        tags = decode(input_len, tagset, score)\n",
    "        fvector = compute_features(tags, input_len, features)           \n",
    "        fvector.times_plus_equal(-1, compute_features(gold_labels, input_len, features))  \n",
    "        return fvector\n",
    "\n",
    "    def training_observer(epoch, parameters):\n",
    "        \"\"\"\n",
    "        Evaluates the parameters on the development data, and writes out the parameters to a 'model.iter'+epoch and\n",
    "        the predictions to 'ner.dev.out'+epoch.\n",
    "        \"\"\"\n",
    "        dev_data = read_data('ner.dev')\n",
    "        (_, _, f1) = evaluate(dev_data, parameters, feature_names, tagset)\n",
    "        write_predictions('ner.dev.out'+str(epoch), dev_data, parameters, feature_names, tagset)\n",
    "        parameters.write_to_file(os.path.join(directory,'model.iter'+str(epoch)))\n",
    "        return f1\n",
    "\n",
    "    return adagrad(len(data), epochs, perceptron_gradient, parameters, training_observer)\n",
    "\n",
    "\n",
    "\n",
    "def predict(inputs, input_len, parameters, feature_names, tagset):\n",
    "    features = Features(inputs, feature_names)\n",
    "\n",
    "    def score(cur_tag, pre_tag, i):\n",
    "        return parameters.dot_product(features.compute_features(cur_tag, pre_tag, i))\n",
    "\n",
    "    return decode(input_len, tagset, score)\n",
    "\n",
    "\n",
    "def make_data_point(sent):\n",
    "    dic = {}\n",
    "    sent = [s.strip().split() for s in sent]\n",
    "    dic['tokens'] = ['<START>'] + [s[0] for s in sent] + ['<STOP>']\n",
    "    dic['pos'] = ['<START>'] + [s[1] for s in sent] + ['<STOP>']\n",
    "    dic['NP_chunk'] = ['<START>'] + [s[2] for s in sent] + ['<STOP>']\n",
    "    dic['gold_tags'] = ['<START>'] + [s[3] for s in sent] + ['<STOP>']\n",
    "    return dic\n",
    "\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        sent = []\n",
    "        for line in f.readlines():\n",
    "            if line.strip():\n",
    "                sent.append(line)\n",
    "            else:\n",
    "                data.append(make_data_point(sent))\n",
    "                sent = []\n",
    "        data.append(make_data_point(sent))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_predictions(out_filename, all_inputs, parameters, feature_names, tagset):\n",
    "    with open(out_filename, 'w', encoding='utf-8') as f:\n",
    "        for inputs in all_inputs:\n",
    "            input_len = len(inputs['tokens'])\n",
    "            tag_seq = predict(inputs, input_len, parameters, feature_names, tagset)\n",
    "            for i, tag in enumerate(tag_seq[1:-1]): \n",
    "                f.write(' '.join([inputs['tokens'][i+1], inputs['pos'][i+1], inputs['NP_chunk'][i+1], inputs['gold_tags'][i+1], tag])+'\\n') # i + 1 because of <START>\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "def evaluate(data, parameters, feature_names, tagset):\n",
    "    all_gold_tags = [ ]\n",
    "    all_predicted_tags = [ ]\n",
    "    for inputs in tqdm(data):\n",
    "        all_gold_tags.extend(inputs['gold_tags'][1:-1])  \n",
    "        input_len = len(inputs['tokens'])\n",
    "        all_predicted_tags.extend(predict(inputs, input_len, parameters, feature_names, tagset)[1:-1])\n",
    "    return conllevaluate(all_gold_tags, all_predicted_tags)\n",
    "\n",
    "def test_decoder():\n",
    "    \n",
    "    tagset = ['NN', 'VB']    \n",
    "\n",
    "    def score_wrap(cur_tag, pre_tag, i):\n",
    "        retval = score(cur_tag, pre_tag, i)\n",
    "        print('Score('+cur_tag+','+pre_tag+','+str(i)+') returning '+str(retval))\n",
    "        return retval\n",
    "\n",
    "    def score(cur_tag, pre_tag, i):\n",
    "        if i == 0:\n",
    "            print(\"ERROR: Don't call score for i = 0 (that points to <START>, with nothing before it)\")\n",
    "        if i == 1:\n",
    "            if pre_tag != '<START>':\n",
    "                print(\"ERROR: Previous tag should be <START> for i = 1. Previous tag = \"+pre_tag)\n",
    "            if cur_tag == 'NN':\n",
    "                return 6\n",
    "            if cur_tag == 'VB':\n",
    "                return 4\n",
    "        if i == 2:\n",
    "            if cur_tag == 'NN' and pre_tag == 'NN':\n",
    "                return 4\n",
    "            if cur_tag == 'NN' and pre_tag == 'VB':\n",
    "                return 9\n",
    "            if cur_tag == 'VB' and pre_tag == 'NN':\n",
    "                return 5\n",
    "            if cur_tag == 'VB' and pre_tag == 'VB':\n",
    "                return 0\n",
    "        if i == 3:\n",
    "            if cur_tag != '<STOP>':\n",
    "                print('ERROR: Current tag at i = 3 should be <STOP>. Current tag = '+cur_tag)\n",
    "            if pre_tag == 'NN':\n",
    "                return 1\n",
    "            if pre_tag == 'VB':\n",
    "                return 1\n",
    "\n",
    "    predicted_tag_seq = decode(4, tagset, score_wrap)\n",
    "    print('Predicted tag sequence should be = <START> VB NN <STOP>')\n",
    "    print('Predicted tag sequence = '+' '.join(predicted_tag_seq))\n",
    "    print(\"Score of ['<START>','VB','NN','<STOP>'] = \"+str(compute_score(['<START>','VB','NN','<STOP>'], 4, score)))\n",
    "    print('Max score should be = 14')\n",
    "    print('Max score = '+str(compute_score(predicted_tag_seq, 4, score)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_predict(data_filename, model_filename, use_four_features=False):\n",
    "    \"\"\"\n",
    "    Main function to make predictions.\n",
    "    Loads the model file and runs the NER tagger on the data, writing the output in CoNLL 2003 evaluation format to data_filename.out\n",
    "    :param data_filename: String\n",
    "    :param model_filename: String\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    data = read_data(data_filename)\n",
    "    parameters = FeatureVector({})\n",
    "    parameters.read_from_file(model_filename)\n",
    "\n",
    "    tagset = ['B-PER', 'B-LOC', 'B-ORG', 'B-MISC', 'I-PER', 'I-LOC', 'I-ORG', 'I-MISC', 'O']\n",
    "\n",
    "    feature_names = ['tag', 'prev_tag', 'current_word', 'curr_pos_tag', 'shape_curr_word', 'len_k', 'in_gazetteer', 'start_cap']\n",
    "\n",
    "    write_predictions(os.path.join(directory, data_filename+'.out'), data, parameters, feature_names, tagset)\n",
    "    evaluate(data, parameters, feature_names, tagset)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def main_train():\n",
    "    \"\"\"\n",
    "    Main function to train the model\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print('Reading training data')\n",
    "    train_data = read_data('ner.train')[:1100]\n",
    "    \n",
    "    tagset = ['B-PER', 'B-LOC', 'B-ORG', 'B-MISC', 'I-PER', 'I-LOC', 'I-ORG', 'I-MISC', 'O']\n",
    "    \n",
    "    feature_names = ['tag', 'prev_tag', 'current_word', 'curr_pos_tag', 'shape_curr_word', 'len_k', 'in_gazetteer', 'start_cap']\n",
    "\n",
    "    \n",
    "    print('Training...')\n",
    "    parameters = train(train_data, feature_names, tagset, epochs=10)\n",
    "    print('Training done')\n",
    "    dev_data = read_data('ner.dev')[:1100]\n",
    "    evaluate(dev_data, parameters, feature_names, tagset)\n",
    "    test_data = read_data('ner.test')[:1100]\n",
    "    \n",
    "    evaluate(test_data, parameters, feature_names, tagset)\n",
    "    parameters.write_to_file('model')\n",
    "\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features(object):\n",
    "    def __init__(self, inputs, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        self.inputs = inputs\n",
    "        self.gazette_dict = {}\n",
    "\n",
    "        with open('gazetteer.txt', 'r') as file:\n",
    "            for row in file:\n",
    "                words = row.split(' ')\n",
    "                value = words[0]\n",
    "                for w in words[1:]:\n",
    "                    if (w in self.gazette_dict.keys()):\n",
    "                        self.gazette_dict[w].append(value)\n",
    "                    else:\n",
    "                        self.gazette_dict[w] = [value]\n",
    "\n",
    "    def compute_features(self, cur_tag, pre_tag, i):\n",
    "        \n",
    "        feats = FeatureVector({})\n",
    "        curr_word = self.inputs['tokens'][i]\n",
    "        len_curr_word = len(self.inputs['tokens'][i])\n",
    "        \n",
    "        if 'tag' in self.feature_names:\n",
    "            feats.times_plus_equal(1, FeatureVector({'t='+cur_tag: 1}))\n",
    "        if 'prev_tag' in self.feature_names:\n",
    "            feats.times_plus_equal(1, FeatureVector({'ti='+cur_tag+\"+ti-1=\"+pre_tag: 1}))\n",
    "        if 'current_word' in self.feature_names:\n",
    "            feats.times_plus_equal(1, FeatureVector({'t='+cur_tag+'+w='+self.inputs['tokens'][i]: 1}))\n",
    "\n",
    "        # adding more features\n",
    "        if 'curr_pos_tag' in self.feature_names:\n",
    "            feats.times_plus_equal(1, FeatureVector({'t='+cur_tag+'+pi='+self.inputs['pos'][i]: 1}))\n",
    "        \n",
    "        if 'shape_curr_word' in self.feature_names:\n",
    "            word_shape = ''.join(['a' if c.isalpha() else 'A' if c.isupper() else 'd' for c in curr_word])\n",
    "            feats.times_plus_equal(1, FeatureVector({'t='+cur_tag+'si'+word_shape: 1}))\n",
    "\n",
    "\n",
    "        if 'len_k' in self.feature_names:\n",
    "            for j in range(1, min(5, len(curr_word) + 1)): \n",
    "                feats.times_plus_equal(1, FeatureVector({'t='+cur_tag+'+PRE'+str(j)+'='+curr_word[:j]: 1}))\n",
    "\n",
    "\n",
    "       \n",
    "        if 'in_gazetteer' in self.feature_names:\n",
    "            if (curr_word) in self.gazette_dict.keys():\n",
    "                if self.gazette_dict[curr_word] == cur_tag:\n",
    "                    feats.times_plus_equal(1, FeatureVector({'t='+cur_tag+'+GAZ='+'True': 1}))\n",
    "            \n",
    "\n",
    "        if 'start_cap' in self.feature_names:\n",
    "            if(curr_word[0].isupper()):\n",
    "                feats.times_plus_equal(1, FeatureVector({'t='+cur_tag+'+CAP='+'True': 1}))\n",
    "        \n",
    "        return feats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureVector(object):\n",
    "\n",
    "    def __init__(self, fdict):\n",
    "        self.fdict = fdict\n",
    "\n",
    "    def times_plus_equal(self, scalar, v2):\n",
    "        \"\"\"\n",
    "        self += scalar * v2\n",
    "        :param scalar: Double\n",
    "        :param v2: FeatureVector\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for key, value in v2.fdict.items():\n",
    "            self.fdict[key] = scalar * value + self.fdict.get(key, 0)\n",
    "        \n",
    "\n",
    "\n",
    "    def power_plus(self, scalar, v2):\n",
    "        \"\"\"\n",
    "        self += scalar * v2,\n",
    "        :param scalar: Double\n",
    "        :param v2: FeatureVector\n",
    "        :return None\n",
    "        \"\"\"\n",
    "        for key, value in v2.fdict.items():\n",
    "            self.fdict[key] = pow(value, scalar) + self.fdict.get(key,0)\n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "    def sqrt_div(self, v2, epsilon):\n",
    "        \"\"\"\n",
    "        self += scalar * v2\n",
    "        :param scalar: Double\n",
    "        :param v2: FeatureVector\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        smoothing = 0.001\n",
    "\n",
    "        for key, value in self.fdict.items():\n",
    "            self.fdict[key] = (epsilon / (math.sqrt(v2.fdict[key]) + smoothing)) * self.fdict.get(key, 0)\n",
    "\n",
    "        return self\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def dot_product(self, v2):\n",
    "        \"\"\"\n",
    "        Computes the dot product between self and v2.  It is more efficient for v2 to be the smaller vector (fewer\n",
    "        non-zero entries).\n",
    "        :param v2: FeatureVector\n",
    "        :return: Int\n",
    "        \"\"\"\n",
    "        retval = 0\n",
    "        for key, value in v2.fdict.items():\n",
    "            retval += value * self.fdict.get(key, 0)\n",
    "        return retval\n",
    "\n",
    "        \n",
    "\n",
    "    def write_to_file(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the feature vector to a file.\n",
    "        :param filename: String\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print('Writing to ' + filename)\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for key, value in self.fdict.items():\n",
    "                f.write('{} {}\\n'.format(key, value))\n",
    "\n",
    "\n",
    "    def read_from_file(self, filename):\n",
    "        \"\"\"\n",
    "        Reads a feature vector from a file.\n",
    "        :param filename: String\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.fdict = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                txt = line.split()\n",
    "                self.fdict[txt[0]] = float(txt[1])\n",
    "\n",
    "#main_train()    # Uncomment to train a model (need to implement 'sgd' function)\n",
    "#main_predict('ner.dev', 'model')  # Uncomment to predict on 'dev.ner' using the model 'model' (need to implement 'decode' function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using Adagrad (trained on 1100 training examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(NN,<START>,1) returning 6\n",
      "Score(VB,<START>,1) returning 4\n",
      "Score(NN,NN,2) returning 4\n",
      "Score(NN,VB,2) returning 9\n",
      "Score(VB,NN,2) returning 5\n",
      "Score(VB,VB,2) returning 0\n",
      "Score(<STOP>,NN,3) returning 1\n",
      "Score(<STOP>,VB,3) returning 1\n",
      "Predicted tag sequence should be = <START> VB NN <STOP>\n",
      "Predicted tag sequence = <START> VB NN <STOP>\n",
      "Score of ['<START>','VB','NN','<STOP>'] = 14\n",
      "Max score should be = 14\n",
      "Max score = 14\n"
     ]
    }
   ],
   "source": [
    "test_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data\n",
      "Training...\n",
      "EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [01:51<00:00,  9.87it/s]\n",
      "100%|██████████| 3466/3466 [03:09<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 8492 phrases; correct: 2357.\n",
      "accuracy:  42.40%; (non-O)\n",
      "accuracy:  82.61%; precision:  27.76%; recall:  39.83%; FB1:  32.72\n",
      "              LOC: precision:  55.70%; recall:  55.03%; FB1:  55.36  1808\n",
      "             MISC: precision:  41.56%; recall:  44.20%; FB1:  42.84  972\n",
      "              ORG: precision:  46.32%; recall:  28.19%; FB1:  35.05  816\n",
      "              PER: precision:  11.60%; recall:  31.00%; FB1:  16.88  4896\n",
      "Writing to adagrad_results/model.iter0\n",
      "32.71566382122285\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [01:51<00:00,  9.84it/s]\n",
      "100%|██████████| 3466/3466 [03:07<00:00, 18.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 7969 phrases; correct: 2359.\n",
      "accuracy:  44.32%; (non-O)\n",
      "accuracy:  83.04%; precision:  29.60%; recall:  39.87%; FB1:  33.98\n",
      "              LOC: precision:  62.37%; recall:  51.53%; FB1:  56.43  1512\n",
      "             MISC: precision:  59.32%; recall:  36.21%; FB1:  44.97  558\n",
      "              ORG: precision:  53.55%; recall:  24.76%; FB1:  33.86  620\n",
      "              PER: precision:  14.26%; recall:  41.10%; FB1:  21.18  5279\n",
      "Writing to adagrad_results/model.iter1\n",
      "33.97666714676652\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [01:51<00:00,  9.83it/s]\n",
      "100%|██████████| 3466/3466 [03:08<00:00, 18.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 7877 phrases; correct: 2256.\n",
      "accuracy:  42.36%; (non-O)\n",
      "accuracy:  82.71%; precision:  28.64%; recall:  38.13%; FB1:  32.71\n",
      "              LOC: precision:  60.27%; recall:  50.49%; FB1:  54.95  1533\n",
      "             MISC: precision:  59.96%; recall:  35.23%; FB1:  44.38  537\n",
      "              ORG: precision:  49.69%; recall:  23.94%; FB1:  32.31  646\n",
      "              PER: precision:  13.35%; recall:  37.61%; FB1:  19.71  5161\n",
      "Writing to adagrad_results/model.iter2\n",
      "32.70987385819922\n",
      "EPOCH 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [01:51<00:00,  9.86it/s]\n",
      "100%|██████████| 3466/3466 [03:07<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 7609 phrases; correct: 2166.\n",
      "accuracy:  39.94%; (non-O)\n",
      "accuracy:  82.35%; precision:  28.47%; recall:  36.61%; FB1:  32.03\n",
      "              LOC: precision:  63.17%; recall:  48.36%; FB1:  54.78  1401\n",
      "             MISC: precision:  62.59%; recall:  36.43%; FB1:  46.06  532\n",
      "              ORG: precision:  52.46%; recall:  23.04%; FB1:  32.02  589\n",
      "              PER: precision:  12.56%; recall:  34.88%; FB1:  18.47  5087\n",
      "Writing to adagrad_results/model.iter3\n",
      "32.02720686086057\n",
      "EPOCH 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [01:50<00:00,  9.91it/s]\n",
      "100%|██████████| 3466/3466 [03:08<00:00, 18.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 7658 phrases; correct: 2136.\n",
      "accuracy:  38.57%; (non-O)\n",
      "accuracy:  82.11%; precision:  27.89%; recall:  36.10%; FB1:  31.47\n",
      "              LOC: precision:  59.04%; recall:  51.04%; FB1:  54.75  1582\n",
      "             MISC: precision:  56.04%; recall:  33.48%; FB1:  41.92  546\n",
      "              ORG: precision:  60.49%; recall:  20.43%; FB1:  30.55  453\n",
      "              PER: precision:  12.25%; recall:  33.95%; FB1:  18.01  5077\n",
      "Writing to adagrad_results/model.iter4\n",
      "31.46961325966851\n",
      "EPOCH 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [01:50<00:00,  9.99it/s]\n",
      "100%|██████████| 3466/3466 [03:02<00:00, 18.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 7596 phrases; correct: 2123.\n",
      "accuracy:  38.59%; (non-O)\n",
      "accuracy:  82.15%; precision:  27.95%; recall:  35.88%; FB1:  31.42\n",
      "              LOC: precision:  61.58%; recall:  50.71%; FB1:  55.62  1507\n",
      "             MISC: precision:  60.63%; recall:  33.70%; FB1:  43.32  508\n",
      "              ORG: precision:  58.04%; recall:  22.07%; FB1:  31.98  510\n",
      "              PER: precision:  11.65%; recall:  32.26%; FB1:  17.12  5071\n",
      "Writing to adagrad_results/model.iter5\n",
      "31.42159402057278\n",
      "EPOCH 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [01:49<00:00, 10.01it/s]\n",
      "100%|██████████| 3466/3466 [03:03<00:00, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 7762 phrases; correct: 2094.\n",
      "accuracy:  37.42%; (non-O)\n",
      "accuracy:  81.93%; precision:  26.98%; recall:  35.39%; FB1:  30.62\n",
      "              LOC: precision:  53.90%; recall:  51.75%; FB1:  52.80  1757\n",
      "             MISC: precision:  59.47%; recall:  34.68%; FB1:  43.81  533\n",
      "              ORG: precision:  59.60%; recall:  20.13%; FB1:  30.10  453\n",
      "              PER: precision:  11.16%; recall:  30.57%; FB1:  16.35  5019\n",
      "Writing to adagrad_results/model.iter6\n",
      "30.61627311938008\n",
      "EPOCH 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [01:51<00:00,  9.89it/s]\n",
      "100%|██████████| 3466/3466 [03:04<00:00, 18.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 7630 phrases; correct: 2057.\n",
      "accuracy:  36.58%; (non-O)\n",
      "accuracy:  81.82%; precision:  26.96%; recall:  34.76%; FB1:  30.37\n",
      "              LOC: precision:  56.79%; recall:  51.86%; FB1:  54.21  1671\n",
      "             MISC: precision:  52.11%; recall:  32.49%; FB1:  40.03  570\n",
      "              ORG: precision:  56.92%; recall:  21.77%; FB1:  31.50  513\n",
      "              PER: precision:  10.64%; recall:  28.33%; FB1:  15.47  4876\n",
      "Writing to adagrad_results/model.iter7\n",
      "30.368347235550303\n",
      "EPOCH 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [01:48<00:00, 10.10it/s]\n",
      "100%|██████████| 3466/3466 [03:02<00:00, 18.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 7443 phrases; correct: 1986.\n",
      "accuracy:  35.12%; (non-O)\n",
      "accuracy:  81.60%; precision:  26.68%; recall:  33.56%; FB1:  29.73\n",
      "              LOC: precision:  58.70%; recall:  49.40%; FB1:  53.65  1540\n",
      "             MISC: precision:  56.55%; recall:  34.03%; FB1:  42.49  550\n",
      "              ORG: precision:  57.58%; recall:  19.84%; FB1:  29.51  462\n",
      "              PER: precision:  10.33%; recall:  27.57%; FB1:  15.02  4891\n",
      "Writing to adagrad_results/model.iter8\n",
      "29.730538922155684\n",
      "EPOCH 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [01:48<00:00, 10.18it/s]\n",
      "100%|██████████| 3466/3466 [03:04<00:00, 18.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 7488 phrases; correct: 1979.\n",
      "accuracy:  35.07%; (non-O)\n",
      "accuracy:  81.58%; precision:  26.43%; recall:  33.45%; FB1:  29.53\n",
      "              LOC: precision:  57.29%; recall:  49.62%; FB1:  53.18  1585\n",
      "             MISC: precision:  58.19%; recall:  33.04%; FB1:  42.15  519\n",
      "              ORG: precision:  53.74%; recall:  20.88%; FB1:  30.08  521\n",
      "              PER: precision:  10.06%; recall:  26.69%; FB1:  14.61  4863\n",
      "Writing to adagrad_results/model.iter9\n",
      "29.526296158149947\n",
      "Training done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dot_product'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mmain_train\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTraining done\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     43\u001b[39m dev_data = read_data(\u001b[33m'\u001b[39m\u001b[33mner.dev\u001b[39m\u001b[33m'\u001b[39m)[:\u001b[32m1100\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m test_data = read_data(\u001b[33m'\u001b[39m\u001b[33mner.test\u001b[39m\u001b[33m'\u001b[39m)[:\u001b[32m1100\u001b[39m]\n\u001b[32m     47\u001b[39m evaluate(test_data, parameters, feature_names, tagset)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 164\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(data, parameters, feature_names, tagset)\u001b[39m\n\u001b[32m    162\u001b[39m     all_gold_tags.extend(inputs[\u001b[33m'\u001b[39m\u001b[33mgold_tags\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m1\u001b[39m:-\u001b[32m1\u001b[39m])  \u001b[38;5;66;03m# deletes <START> and <STOP>\u001b[39;00m\n\u001b[32m    163\u001b[39m     input_len = \u001b[38;5;28mlen\u001b[39m(inputs[\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     all_predicted_tags.extend(\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m:-\u001b[32m1\u001b[39m]) \u001b[38;5;66;03m# deletes <START> and <STOP>\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m conllevaluate(all_gold_tags, all_predicted_tags)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mpredict\u001b[39m\u001b[34m(inputs, input_len, parameters, feature_names, tagset)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscore\u001b[39m(cur_tag, pre_tag, i):\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parameters.dot_product(features.compute_features(cur_tag, pre_tag, i))\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(input_length, tagset, score, debug)\u001b[39m\n\u001b[32m     20\u001b[39m best_path = []\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tagset):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     viterbi[i][\u001b[32m1\u001b[39m] = \u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<START>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug: \u001b[38;5;28mprint\u001b[39m(viterbi)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2\u001b[39m, input_length - \u001b[32m1\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mpredict.<locals>.score\u001b[39m\u001b[34m(cur_tag, pre_tag, i)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscore\u001b[39m(cur_tag, pre_tag, i):\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparameters\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot_product\u001b[49m(features.compute_features(cur_tag, pre_tag, i))\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'dot_product'"
     ]
    }
   ],
   "source": [
    "main_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3466/3466 [03:12<00:00, 18.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 7488 phrases; correct: 1979.\n",
      "accuracy:  35.07%; (non-O)\n",
      "accuracy:  81.58%; precision:  26.43%; recall:  33.45%; FB1:  29.53\n",
      "              LOC: precision:  57.29%; recall:  49.62%; FB1:  53.18  1585\n",
      "             MISC: precision:  58.19%; recall:  33.04%; FB1:  42.15  519\n",
      "              ORG: precision:  53.74%; recall:  20.88%; FB1:  30.08  521\n",
      "              PER: precision:  10.06%; recall:  26.69%; FB1:  14.61  4863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main_predict('ner.dev', 'adagrad_results/model.iter9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3684/3684 [03:21<00:00, 18.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46666 tokens with 5616 phrases; found: 7763 phrases; correct: 1711.\n",
      "accuracy:  32.94%; (non-O)\n",
      "accuracy:  79.03%; precision:  22.04%; recall:  30.47%; FB1:  25.58\n",
      "              LOC: precision:  54.16%; recall:  51.62%; FB1:  52.86  1588\n",
      "             MISC: precision:  40.44%; recall:  23.82%; FB1:  29.98  413\n",
      "              ORG: precision:  46.77%; recall:  16.27%; FB1:  24.14  573\n",
      "              PER: precision:   8.02%; recall:  25.97%; FB1:  12.25  5189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main_predict('ner.test', 'adagrad_results/model.iter9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHEtAu_LfmPG"
   },
   "outputs": [],
   "source": [
    "!cat \"results/model.iter6\" | awk '{print $2, $1}' | sort -gr > \"results/model.sorted.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bS4d4Acfp1S"
   },
   "source": [
    "The file `model.sorted.txt` will be viewable in your Google Drive folder."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP-202_A1_Code.ipynb",
   "provenance": [
    {
     "file_id": "186hCS3cdEtl0vpkgCXHYgLUu-BkFZZ9T",
     "timestamp": 1583157228957
    }
   ]
  },
  "kernelspec": {
   "display_name": "203A3E1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
