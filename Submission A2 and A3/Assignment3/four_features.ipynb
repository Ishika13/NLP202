{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHdbUbDpdTBp"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from conlleval import evaluate as conllevaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "directory = 'results/'\n",
    "\n",
    "def decode(input_length, tagset, score, debug=False):\n",
    "    viterbi = [[0 for _ in range(input_length)] for _ in range(len(tagset))]\n",
    "    backpointer = [[0 for _ in range(input_length)] for _ in range(len(tagset))]\n",
    "    best_path = []\n",
    "\n",
    "    for i, tag in enumerate(tagset):\n",
    "        viterbi[i][1] = score(tag, \"<START>\", 1)\n",
    "    \n",
    "    if debug: print(viterbi)\n",
    "        \n",
    "    for t in range(2, input_length - 1):\n",
    "        for s, tag in enumerate(tagset):\n",
    "            max_val = 0\n",
    "            max_index = 0\n",
    "            for b, prev_tag in enumerate(tagset):\n",
    "                curr_val = viterbi[b][t - 1] + score(tag, prev_tag, t)\n",
    "                if curr_val > max_val:\n",
    "                    max_val = curr_val\n",
    "                    max_index = b\n",
    "            viterbi[s][t] = max_val\n",
    "            backpointer[s][t] = max_index\n",
    "\n",
    "    # Calculate the last column of viterbi matrix\n",
    "    for i, tag in enumerate(tagset):\n",
    "        viterbi[i][input_length - 1] = viterbi[i][input_length - 2] + score(\"<STOP>\", tag, input_length - 1)\n",
    "\n",
    "    # Find the best path probability in the last column\n",
    "    best_path_prob = 0\n",
    "    index_to_best_path = 0\n",
    "    for i in range(len(tagset)):\n",
    "        if viterbi[i][-1] > best_path_prob:\n",
    "            best_path_prob = viterbi[i][-1]\n",
    "            index_to_best_path = i\n",
    "\n",
    "    # Retrieve the best path\n",
    "    best_path = [\"<STOP>\"]\n",
    "    for i in range(input_length - 1, 0, -1):\n",
    "        if i == 1:\n",
    "            best_path.insert(0, \"<START>\")\n",
    "        else:\n",
    "            index_to_best_path = backpointer[index_to_best_path][i]\n",
    "            best_path.insert(0, tagset[index_to_best_path])\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(tag_seq, input_length, score):\n",
    "    \"\"\"\n",
    "    Computes the total score of a tag sequence \n",
    "    \"\"\"\n",
    "    total_score = 0\n",
    "    for i in range(1, input_length):\n",
    "        total_score += score(tag_seq[i], tag_seq[i - 1], i)\n",
    "    return total_score\n",
    "\n",
    "\n",
    "def compute_features(tag_seq, input_length, features):\n",
    "    \"\"\"\n",
    "    Compute f(xi, yi)\n",
    "    \"\"\"\n",
    "    feats = FeatureVector({})\n",
    "    for i in range(1, input_length):\n",
    "        feats.times_plus_equal(1, features.compute_features(tag_seq[i], tag_seq[i - 1], i))\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.path.exists('ner.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(training_size, epochs, gradient, parameters, training_observer):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent\n",
    "    \"\"\"\n",
    "    for i in range(epochs):\n",
    "        print(\"epoch: \", i)\n",
    "        indices = [i for i in range(training_size)]\n",
    "        random.shuffle(indices)\n",
    "        for t in tqdm(indices):\n",
    "            parameters.times_plus_equal(-1, gradient(t))\n",
    "        print(\"Running the training observer\")\n",
    "        training_observer(i, parameters)\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def train(data, feature_names, tagset, epochs):\n",
    "    \"\"\"\n",
    "    Trains the model on the data and returns the parameters\n",
    "    \"\"\"\n",
    "    parameters = FeatureVector({})  \n",
    "\n",
    "    def perceptron_gradient(i):\n",
    "        inputs = data[i]\n",
    "        input_len = len(inputs['tokens'])\n",
    "        gold_labels = inputs['gold_tags']\n",
    "        features = Features(inputs, feature_names)\n",
    "\n",
    "        \n",
    "        def score(cur_tag, pre_tag, i):\n",
    "            return parameters.dot_product(features.compute_features(cur_tag, pre_tag, i))\n",
    "\n",
    "        tags = decode(input_len, tagset, score)\n",
    "\n",
    "        fvector = compute_features(tags, input_len, features)       \n",
    "        fvector.times_plus_equal(-1, compute_features(gold_labels, input_len, features))   \n",
    "        return fvector\n",
    "\n",
    "    def training_observer(epoch, parameters):\n",
    "        \"\"\"\n",
    "        Evaluates the parameters on the development data, and writes out the parameters to a 'model.iter'+epoch and\n",
    "        the predictions to 'ner.dev.out'+epoch.\n",
    "        \"\"\"\n",
    "        dev_data = read_data('ner.dev')[:1000]\n",
    "        (_, _, f1) = evaluate(dev_data, parameters, feature_names, tagset)\n",
    "        write_predictions(os.path.join(directory,'ner.dev.out'+str(epoch)), dev_data, parameters, feature_names, tagset)\n",
    "        parameters.write_to_file(os.path.join(directory, 'model.iter'+str(epoch)))\n",
    "        return f1\n",
    "\n",
    "    \n",
    "    return sgd(len(data), epochs, perceptron_gradient, parameters, training_observer)\n",
    "\n",
    "\n",
    "def predict(inputs, input_len, parameters, feature_names, tagset):\n",
    "    features = Features(inputs, feature_names)\n",
    "    def score(cur_tag, pre_tag, i):\n",
    "        return parameters.dot_product(features.compute_features(cur_tag, pre_tag, i))\n",
    "\n",
    "    return decode(input_len, tagset, score)\n",
    "\n",
    "\n",
    "def make_data_point(sent):\n",
    "    dic = {}\n",
    "    sent = [s.strip().split() for s in sent]\n",
    "    dic['tokens'] = ['<START>'] + [s[0] for s in sent] + ['<STOP>']\n",
    "    dic['pos'] = ['<START>'] + [s[1] for s in sent] + ['<STOP>']\n",
    "    dic['NP_chunk'] = ['<START>'] + [s[2] for s in sent] + ['<STOP>']\n",
    "    dic['gold_tags'] = ['<START>'] + [s[3] for s in sent] + ['<STOP>']\n",
    "    return dic\n",
    "\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        sent = []\n",
    "        for line in f.readlines():\n",
    "            if line.strip():\n",
    "                sent.append(line)\n",
    "            else:\n",
    "                data.append(make_data_point(sent))\n",
    "                sent = []\n",
    "        data.append(make_data_point(sent))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_predictions(out_filename, all_inputs, parameters, feature_names, tagset):\n",
    "    with open(out_filename, 'w', encoding='utf-8') as f:\n",
    "        for inputs in all_inputs:\n",
    "            input_len = len(inputs['tokens'])\n",
    "            tag_seq = predict(inputs, input_len, parameters, feature_names, tagset)\n",
    "            for i, tag in enumerate(tag_seq[1:-1]):  \n",
    "                f.write(' '.join([inputs['tokens'][i+1], inputs['pos'][i+1], inputs['NP_chunk'][i+1], inputs['gold_tags'][i+1], tag])+'\\n') \n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "def evaluate(data, parameters, feature_names, tagset):\n",
    "    all_gold_tags = [ ]\n",
    "    all_predicted_tags = [ ]\n",
    "    for inputs in tqdm(data):\n",
    "        all_gold_tags.extend(inputs['gold_tags'][1:-1])  \n",
    "        input_len = len(inputs['tokens'])\n",
    "        all_predicted_tags.extend(predict(inputs, input_len, parameters, feature_names, tagset)[1:-1]) \n",
    "    return conllevaluate(all_gold_tags, all_predicted_tags)\n",
    "\n",
    "def test_decoder():\n",
    "    \n",
    "    tagset = ['NN', 'VB']    \n",
    "\n",
    "    def score_wrap(cur_tag, pre_tag, i):\n",
    "        retval = score(cur_tag, pre_tag, i)\n",
    "        print('Score('+cur_tag+','+pre_tag+','+str(i)+') returning '+str(retval))\n",
    "        return retval\n",
    "\n",
    "    def score(cur_tag, pre_tag, i):\n",
    "        if i == 0:\n",
    "            print(\"ERROR: Don't call score for i = 0 (that points to <START>, with nothing before it)\")\n",
    "        if i == 1:\n",
    "            if pre_tag != '<START>':\n",
    "                print(\"ERROR: Previous tag should be <START> for i = 1. Previous tag = \"+pre_tag)\n",
    "            if cur_tag == 'NN':\n",
    "                return 6\n",
    "            if cur_tag == 'VB':\n",
    "                return 4\n",
    "        if i == 2:\n",
    "            if cur_tag == 'NN' and pre_tag == 'NN':\n",
    "                return 4\n",
    "            if cur_tag == 'NN' and pre_tag == 'VB':\n",
    "                return 9\n",
    "            if cur_tag == 'VB' and pre_tag == 'NN':\n",
    "                return 5\n",
    "            if cur_tag == 'VB' and pre_tag == 'VB':\n",
    "                return 0\n",
    "        if i == 3:\n",
    "            if cur_tag != '<STOP>':\n",
    "                print('ERROR: Current tag at i = 3 should be <STOP>. Current tag = '+cur_tag)\n",
    "            if pre_tag == 'NN':\n",
    "                return 1\n",
    "            if pre_tag == 'VB':\n",
    "                return 1\n",
    "\n",
    "    predicted_tag_seq = decode(4, tagset, score_wrap)\n",
    "    print('Predicted tag sequence should be = <START> VB NN <STOP>')\n",
    "    print('Predicted tag sequence = '+' '.join(predicted_tag_seq))\n",
    "    print(\"Score of ['<START>','VB','NN','<STOP>'] = \"+str(compute_score(['<START>','VB','NN','<STOP>'], 4, score)))\n",
    "    print('Max score should be = 14')\n",
    "    print('Max score = '+str(compute_score(predicted_tag_seq, 4, score)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_predict(data_filename, model_filename, use_four_features=False):\n",
    "    \"\"\"\n",
    "    Main function to make predictions.\n",
    "    \"\"\"\n",
    "    data = read_data(data_filename)\n",
    "    parameters = FeatureVector({})\n",
    "    parameters.read_from_file(model_filename)\n",
    "\n",
    "    tagset = ['B-PER', 'B-LOC', 'B-ORG', 'B-MISC', 'I-PER', 'I-LOC', 'I-ORG', 'I-MISC', 'O']\n",
    "\n",
    "    feature_names = ['tag', 'prev_tag', 'current_word', 'curr_pos_tag']\n",
    "\n",
    "    write_predictions(data_filename+'.out', data, parameters, feature_names, tagset)\n",
    "    evaluate(data, parameters, feature_names, tagset)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def main_train():\n",
    "    \"\"\"\n",
    "    Main function to train the model\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print('Reading training data')\n",
    "    train_data = read_data('ner.train')[:1100]\n",
    "    \n",
    "    tagset = ['B-PER', 'B-LOC', 'B-ORG', 'B-MISC', 'I-PER', 'I-LOC', 'I-ORG', 'I-MISC', 'O']\n",
    "    \n",
    "    feature_names = ['tag', 'prev_tag', 'current_word', 'curr_pos_tag']\n",
    "    \n",
    "    print('Training...')\n",
    "    parameters = train(train_data, feature_names, tagset, epochs=10)\n",
    "    print('Training done')\n",
    "    dev_data = read_data('ner.dev')[:1100]\n",
    "    evaluate(dev_data, parameters, feature_names, tagset)\n",
    "    test_data = read_data('ner.test')[:1100]\n",
    "    \n",
    "    evaluate(test_data, parameters, feature_names, tagset)\n",
    "    parameters.write_to_file('model')\n",
    "\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features(object):\n",
    "    def __init__(self, inputs, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        self.inputs = inputs\n",
    "        self.gazette_dict = {}\n",
    "\n",
    "        with open('gazetteer.txt', 'r') as file:\n",
    "            for row in file:\n",
    "                words = row.split(' ')\n",
    "                value = words[0]\n",
    "                for w in words[1:]:\n",
    "                    if (w in self.gazette_dict.keys()):\n",
    "                        self.gazette_dict[w].append(value)\n",
    "                    else:\n",
    "                        self.gazette_dict[w] = [value]\n",
    "\n",
    "    def compute_features(self, cur_tag, pre_tag, i):\n",
    "        \n",
    "        feats = FeatureVector({})\n",
    "        curr_word = self.inputs['tokens'][i]\n",
    "        len_curr_word = len(self.inputs['tokens'][i])\n",
    "        \n",
    "        if 'tag' in self.feature_names:\n",
    "            feats.times_plus_equal(1, FeatureVector({'t='+cur_tag: 1}))\n",
    "        if 'prev_tag' in self.feature_names:\n",
    "            feats.times_plus_equal(1, FeatureVector({'ti='+cur_tag+\"+ti-1=\"+pre_tag: 1}))\n",
    "        if 'current_word' in self.feature_names:\n",
    "            feats.times_plus_equal(1, FeatureVector({'t='+cur_tag+'+w='+self.inputs['tokens'][i]: 1}))\n",
    "\n",
    "        # adding more features\n",
    "        if 'curr_pos_tag' in self.feature_names:\n",
    "            feats.times_plus_equal(1, FeatureVector({'t='+cur_tag+'+pi='+self.inputs['pos'][i]: 1}))\n",
    "        \n",
    "        # \n",
    "            \n",
    "        #     if 'shape_curr_word' in self.feature_names:\n",
    "        #         word_shape = ''.join(['a' if c.isalpha() else 'A' if c.isupper() else 'd' for c in curr_word])\n",
    "        #         feats.times_plus_equal(1, FeatureVector({'t='+cur_tag+'si'+word_shape: 1}))\n",
    "    \n",
    "\n",
    "        #     if 'len_k' in self.feature_names:\n",
    "        #         for j in range(1, min(5, len(curr_word) + 1)): \n",
    "        #             feats.times_plus_equal(1, FeatureVector({'t='+cur_tag+'+PRE'+str(j)+'='+curr_word[:j]: 1}))\n",
    "\n",
    "    \n",
    "           \n",
    "        #     if 'in_gazetteer' in self.feature_names:\n",
    "        #         if (curr_word) in self.gazette_dict.keys():\n",
    "        #             if self.gazette_dict[curr_word] == cur_tag:\n",
    "        #                 feats.times_plus_equal(1, FeatureVector({'t='+cur_tag+'+GAZ='+'True': 1}))\n",
    "                \n",
    "\n",
    "        #     if 'start_cap' in self.feature_names:\n",
    "        #         if(curr_word[0].isupper()):\n",
    "        #             feats.times_plus_equal(1, FeatureVector({'t='+cur_tag+'+CAP='+'True': 1}))\n",
    "            \n",
    "        return feats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data\n",
      "Training...\n",
      "epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 24.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 23.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2815 phrases; correct: 587.\n",
      "accuracy:  47.89%; (non-O)\n",
      "accuracy:  77.15%; precision:  20.85%; recall:  35.43%; FB1:  26.25\n",
      "              LOC: precision:  95.52%; recall:  36.09%; FB1:  52.39  201\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  309\n",
      "              ORG: precision:  66.67%; recall:   0.61%; FB1:   1.20  3\n",
      "              PER: precision:  17.07%; recall:  63.29%; FB1:  26.89  2302\n",
      "Writing to results/model.iter0\n",
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 24.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 23.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 3057 phrases; correct: 394.\n",
      "accuracy:  32.00%; (non-O)\n",
      "accuracy:  75.98%; precision:  12.89%; recall:  23.78%; FB1:  16.72\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  1\n",
      "             MISC: precision:   2.53%; recall:  19.43%; FB1:   4.47  1345\n",
      "              ORG: precision:  31.36%; recall:  16.11%; FB1:  21.29  169\n",
      "              PER: precision:  19.91%; recall:  49.44%; FB1:  28.39  1542\n",
      "Writing to results/model.iter1\n",
      "epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 24.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 23.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2958 phrases; correct: 493.\n",
      "accuracy:  28.32%; (non-O)\n",
      "accuracy:  75.98%; precision:  16.67%; recall:  29.75%; FB1:  21.37\n",
      "              LOC: precision:  32.13%; recall:  67.11%; FB1:  43.46  1111\n",
      "             MISC: precision:   7.10%; recall:   7.43%; FB1:   7.26  183\n",
      "              ORG: precision:  15.66%; recall:  18.84%; FB1:  17.10  396\n",
      "              PER: precision:   4.81%; recall:   9.82%; FB1:   6.46  1268\n",
      "Writing to results/model.iter2\n",
      "epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 24.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 23.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2654 phrases; correct: 533.\n",
      "accuracy:  34.76%; (non-O)\n",
      "accuracy:  78.84%; precision:  20.08%; recall:  32.17%; FB1:  24.73\n",
      "              LOC: precision:  87.25%; recall:  41.17%; FB1:  55.94  251\n",
      "             MISC: precision:   6.06%; recall:   2.29%; FB1:   3.32  66\n",
      "              ORG: precision:  16.61%; recall:  47.42%; FB1:  24.61  939\n",
      "              PER: precision:  11.02%; recall:  24.80%; FB1:  15.26  1398\n",
      "Writing to results/model.iter3\n",
      "epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 24.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 23.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2973 phrases; correct: 483.\n",
      "accuracy:  25.85%; (non-O)\n",
      "accuracy:  74.15%; precision:  16.25%; recall:  29.15%; FB1:  20.86\n",
      "              LOC: precision:  50.16%; recall:  59.77%; FB1:  54.55  634\n",
      "             MISC: precision:   0.96%; recall:   1.14%; FB1:   1.04  208\n",
      "              ORG: precision:  17.10%; recall:  48.02%; FB1:  25.22  924\n",
      "              PER: precision:   0.41%; recall:   0.81%; FB1:   0.55  1207\n",
      "Writing to results/model.iter4\n",
      "epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:46<00:00, 23.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:42<00:00, 23.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 1921 phrases; correct: 324.\n",
      "accuracy:  23.62%; (non-O)\n",
      "accuracy:  77.35%; precision:  16.87%; recall:  19.55%; FB1:  18.11\n",
      "              LOC: precision:  86.36%; recall:  39.29%; FB1:  54.01  242\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  50\n",
      "              ORG: precision:  35.87%; recall:  10.03%; FB1:  15.68  92\n",
      "              PER: precision:   5.34%; recall:  13.20%; FB1:   7.60  1537\n",
      "Writing to results/model.iter5\n",
      "epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 23.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2656 phrases; correct: 739.\n",
      "accuracy:  50.12%; (non-O)\n",
      "accuracy:  81.49%; precision:  27.82%; recall:  44.60%; FB1:  34.27\n",
      "              LOC: precision:  46.38%; recall:  63.91%; FB1:  53.75  733\n",
      "             MISC: precision:   6.48%; recall:   8.00%; FB1:   7.16  216\n",
      "              ORG: precision:  53.33%; recall:  12.16%; FB1:  19.80  75\n",
      "              PER: precision:  21.14%; recall:  55.56%; FB1:  30.63  1632\n",
      "Writing to results/model.iter6\n",
      "epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:46<00:00, 23.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 23.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2742 phrases; correct: 797.\n",
      "accuracy:  54.62%; (non-O)\n",
      "accuracy:  82.05%; precision:  29.07%; recall:  48.10%; FB1:  36.24\n",
      "              LOC: precision:  85.92%; recall:  44.74%; FB1:  58.84  277\n",
      "             MISC: precision:  13.54%; recall:   7.43%; FB1:   9.59  96\n",
      "              ORG: precision:  44.61%; recall:  27.66%; FB1:  34.15  204\n",
      "              PER: precision:  21.02%; recall:  73.27%; FB1:  32.66  2165\n",
      "Writing to results/model.iter7\n",
      "epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 24.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 23.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2736 phrases; correct: 482.\n",
      "accuracy:  28.82%; (non-O)\n",
      "accuracy:  77.69%; precision:  17.62%; recall:  29.09%; FB1:  21.94\n",
      "              LOC: precision:  32.00%; recall:  63.35%; FB1:  42.52  1053\n",
      "             MISC: precision:   5.97%; recall:   2.29%; FB1:   3.31  67\n",
      "              ORG: precision:  37.22%; recall:  25.23%; FB1:  30.07  223\n",
      "              PER: precision:   4.16%; recall:   9.34%; FB1:   5.76  1393\n",
      "Writing to results/model.iter8\n",
      "epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 23.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 23.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2626 phrases; correct: 420.\n",
      "accuracy:  24.48%; (non-O)\n",
      "accuracy:  76.96%; precision:  15.99%; recall:  25.35%; FB1:  19.61\n",
      "              LOC: precision:  30.83%; recall:  63.91%; FB1:  41.59  1103\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  59\n",
      "              ORG: precision:  40.98%; recall:   7.60%; FB1:  12.82  61\n",
      "              PER: precision:   3.92%; recall:   8.86%; FB1:   5.43  1403\n",
      "Writing to results/model.iter9\n",
      "Training done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:46<00:00, 23.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 15257 tokens with 1808 phrases; found: 2868 phrases; correct: 446.\n",
      "accuracy:  24.14%; (non-O)\n",
      "accuracy:  78.48%; precision:  15.55%; recall:  24.67%; FB1:  19.08\n",
      "              LOC: precision:  29.85%; recall:  62.16%; FB1:  40.33  1216\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  59\n",
      "              ORG: precision:  41.54%; recall:   7.48%; FB1:  12.68  65\n",
      "              PER: precision:   3.66%; recall:   8.43%; FB1:   5.11  1528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 24.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13063 tokens with 1888 phrases; found: 3000 phrases; correct: 367.\n",
      "accuracy:  20.70%; (non-O)\n",
      "accuracy:  72.31%; precision:  12.23%; recall:  19.44%; FB1:  15.02\n",
      "              LOC: precision:  23.96%; recall:  58.47%; FB1:  33.99  1152\n",
      "             MISC: precision:  10.00%; recall:   4.39%; FB1:   6.10  90\n",
      "              ORG: precision:  43.90%; recall:   3.31%; FB1:   6.16  41\n",
      "              PER: precision:   3.73%; recall:   9.58%; FB1:   5.37  1717\n",
      "Writing to model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3466/3466 [02:28<00:00, 23.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 9101 phrases; correct: 1375.\n",
      "accuracy:  22.89%; (non-O)\n",
      "accuracy:  79.02%; precision:  15.11%; recall:  23.24%; FB1:  18.31\n",
      "              LOC: precision:  29.44%; recall:  60.33%; FB1:  39.57  3750\n",
      "             MISC: precision:   7.56%; recall:   1.86%; FB1:   2.99  225\n",
      "              ORG: precision:  44.02%; recall:   6.04%; FB1:  10.62  184\n",
      "              PER: precision:   3.50%; recall:   9.44%; FB1:   5.11  4942\n"
     ]
    }
   ],
   "source": [
    "class FeatureVector(object):\n",
    "\n",
    "    def __init__(self, fdict):\n",
    "        self.fdict = fdict\n",
    "\n",
    "    def times_plus_equal(self, scalar, v2):\n",
    "        try: \n",
    "            for key, value in v2.fdict.items():\n",
    "                self.fdict[key] = scalar * value + self.fdict.get(key, 0)\n",
    "        except:\n",
    "            print(v2)\n",
    "\n",
    "    def dot_product(self, v2):\n",
    "        retval = 0\n",
    "        for key, value in v2.fdict.items():\n",
    "            retval += value * self.fdict.get(key, 0)\n",
    "        return retval\n",
    "\n",
    "    def write_to_file(self, filename):\n",
    "        print('Writing to ' + filename)\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for key, value in self.fdict.items():\n",
    "                f.write('{} {}\\n'.format(key, value))\n",
    "\n",
    "\n",
    "    def read_from_file(self, filename):\n",
    "        self.fdict = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                txt = line.split()\n",
    "                self.fdict[txt[0]] = float(txt[1])\n",
    "\n",
    "main_train()   \n",
    "main_predict('ner.dev', 'model')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(NN,<START>,1) returning 6\n",
      "Score(VB,<START>,1) returning 4\n",
      "Score(NN,NN,2) returning 4\n",
      "Score(NN,VB,2) returning 9\n",
      "Score(VB,NN,2) returning 5\n",
      "Score(VB,VB,2) returning 0\n",
      "Score(<STOP>,NN,3) returning 1\n",
      "Score(<STOP>,VB,3) returning 1\n",
      "Predicted tag sequence should be = <START> VB NN <STOP>\n",
      "Predicted tag sequence = <START> VB NN <STOP>\n",
      "Score of ['<START>','VB','NN','<STOP>'] = 14\n",
      "Max score should be = 14\n",
      "Max score = 14\n"
     ]
    }
   ],
   "source": [
    "test_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using SGD (trained on 1100 training examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data\n",
      "Training...\n",
      "epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 24.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 23.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2053 phrases; correct: 540.\n",
      "accuracy:  40.38%; (non-O)\n",
      "accuracy:  79.94%; precision:  26.30%; recall:  32.59%; FB1:  29.11\n",
      "              LOC: precision:  91.76%; recall:  31.39%; FB1:  46.78  182\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  42\n",
      "              ORG: precision:  47.83%; recall:   3.34%; FB1:   6.25  23\n",
      "              PER: precision:  20.04%; recall:  58.29%; FB1:  29.83  1806\n",
      "Writing to results/model.iter0\n",
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 23.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2812 phrases; correct: 451.\n",
      "accuracy:  24.36%; (non-O)\n",
      "accuracy:  76.20%; precision:  16.04%; recall:  27.22%; FB1:  20.18\n",
      "              LOC: precision:  29.51%; recall:  63.35%; FB1:  40.26  1142\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  48\n",
      "              ORG: precision:  22.22%; recall:  26.75%; FB1:  24.28  396\n",
      "              PER: precision:   2.12%; recall:   4.19%; FB1:   2.82  1226\n",
      "Writing to results/model.iter1\n",
      "epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:44<00:00, 24.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 23.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 1488 phrases; correct: 107.\n",
      "accuracy:   5.41%; (non-O)\n",
      "accuracy:  74.01%; precision:   7.19%; recall:   6.46%; FB1:   6.80\n",
      "              LOC: precision:  79.66%; recall:  17.67%; FB1:  28.92  118\n",
      "             MISC: precision:   0.65%; recall:   0.57%; FB1:   0.61  155\n",
      "              ORG: precision:  56.25%; recall:   2.74%; FB1:   5.22  16\n",
      "              PER: precision:   0.25%; recall:   0.48%; FB1:   0.33  1199\n",
      "Writing to results/model.iter2\n",
      "epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 23.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 1721 phrases; correct: 245.\n",
      "accuracy:  14.62%; (non-O)\n",
      "accuracy:  75.68%; precision:  14.24%; recall:  14.79%; FB1:  14.51\n",
      "              LOC: precision:  86.85%; recall:  34.77%; FB1:  49.66  213\n",
      "             MISC: precision:   5.56%; recall:   2.29%; FB1:   3.24  72\n",
      "              ORG: precision:  22.16%; recall:  13.07%; FB1:  16.44  194\n",
      "              PER: precision:   1.05%; recall:   2.09%; FB1:   1.40  1242\n",
      "Writing to results/model.iter3\n",
      "epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 24.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2788 phrases; correct: 713.\n",
      "accuracy:  50.74%; (non-O)\n",
      "accuracy:  80.38%; precision:  25.57%; recall:  43.03%; FB1:  32.08\n",
      "              LOC: precision:  93.88%; recall:  34.59%; FB1:  50.55  196\n",
      "             MISC: precision:  12.24%; recall:   6.86%; FB1:   8.79  98\n",
      "              ORG: precision:  41.89%; recall:  18.84%; FB1:  26.00  148\n",
      "              PER: precision:  19.39%; recall:  73.27%; FB1:  30.67  2346\n",
      "Writing to results/model.iter4\n",
      "epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:44<00:00, 24.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2629 phrases; correct: 685.\n",
      "accuracy:  49.96%; (non-O)\n",
      "accuracy:  80.58%; precision:  26.06%; recall:  41.34%; FB1:  31.96\n",
      "              LOC: precision:  40.46%; recall:  65.79%; FB1:  50.11  865\n",
      "             MISC: precision:   5.36%; recall:   1.71%; FB1:   2.60  56\n",
      "              ORG: precision:  92.86%; recall:   3.95%; FB1:   7.58  14\n",
      "              PER: precision:  18.83%; recall:  51.37%; FB1:  27.56  1694\n",
      "Writing to results/model.iter5\n",
      "epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:44<00:00, 24.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:44<00:00, 22.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2601 phrases; correct: 700.\n",
      "accuracy:  48.43%; (non-O)\n",
      "accuracy:  80.94%; precision:  26.91%; recall:  42.25%; FB1:  32.88\n",
      "              LOC: precision:  85.14%; recall:  44.17%; FB1:  58.17  276\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  41\n",
      "              ORG: precision:  57.14%; recall:   3.65%; FB1:   6.86  21\n",
      "              PER: precision:  20.02%; recall:  72.95%; FB1:  31.41  2263\n",
      "Writing to results/model.iter6\n",
      "epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:46<00:00, 23.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2704 phrases; correct: 503.\n",
      "accuracy:  32.04%; (non-O)\n",
      "accuracy:  78.20%; precision:  18.60%; recall:  30.36%; FB1:  23.07\n",
      "              LOC: precision:  34.64%; recall:  62.97%; FB1:  44.70  967\n",
      "             MISC: precision:  12.75%; recall:   7.43%; FB1:   9.39  102\n",
      "              ORG: precision:  51.72%; recall:  22.80%; FB1:  31.65  145\n",
      "              PER: precision:   5.37%; recall:  12.88%; FB1:   7.58  1490\n",
      "Writing to results/model.iter7\n",
      "epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:44<00:00, 24.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 1995 phrases; correct: 476.\n",
      "accuracy:  32.04%; (non-O)\n",
      "accuracy:  78.68%; precision:  23.86%; recall:  28.73%; FB1:  26.07\n",
      "              LOC: precision:  81.79%; recall:  44.74%; FB1:  57.84  291\n",
      "             MISC: precision:  13.98%; recall:   7.43%; FB1:   9.70  93\n",
      "              ORG: precision:  29.89%; recall:  16.72%; FB1:  21.44  184\n",
      "              PER: precision:  11.91%; recall:  27.38%; FB1:  16.60  1427\n",
      "Writing to results/model.iter8\n",
      "epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:44<00:00, 24.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the training observer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13018 tokens with 1657 phrases; found: 2655 phrases; correct: 512.\n",
      "accuracy:  30.76%; (non-O)\n",
      "accuracy:  78.06%; precision:  19.28%; recall:  30.90%; FB1:  23.75\n",
      "              LOC: precision:  33.96%; recall:  61.28%; FB1:  43.70  960\n",
      "             MISC: precision:   7.93%; recall:   7.43%; FB1:   7.67  164\n",
      "              ORG: precision:  27.75%; recall:  16.11%; FB1:  20.38  191\n",
      "              PER: precision:   8.96%; recall:  19.32%; FB1:  12.24  1340\n",
      "Writing to results/model.iter9\n",
      "Training done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 24.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 15257 tokens with 1808 phrases; found: 2900 phrases; correct: 539.\n",
      "accuracy:  29.72%; (non-O)\n",
      "accuracy:  79.37%; precision:  18.59%; recall:  29.81%; FB1:  22.90\n",
      "              LOC: precision:  32.89%; recall:  59.59%; FB1:  42.39  1058\n",
      "             MISC: precision:   8.19%; recall:   7.04%; FB1:   7.57  171\n",
      "              ORG: precision:  25.94%; recall:  15.24%; FB1:  19.20  212\n",
      "              PER: precision:   8.36%; recall:  18.37%; FB1:  11.49  1459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:45<00:00, 24.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13063 tokens with 1888 phrases; found: 3026 phrases; correct: 450.\n",
      "accuracy:  25.12%; (non-O)\n",
      "accuracy:  73.08%; precision:  14.87%; recall:  23.83%; FB1:  18.32\n",
      "              LOC: precision:  26.54%; recall:  57.63%; FB1:  36.34  1025\n",
      "             MISC: precision:  10.17%; recall:   8.78%; FB1:   9.42  177\n",
      "              ORG: precision:  21.86%; recall:   8.66%; FB1:  12.40  215\n",
      "              PER: precision:   7.02%; recall:  16.92%; FB1:   9.93  1609\n",
      "Writing to model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3466/3466 [02:37<00:00, 21.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5917 phrases; found: 7002 phrases; correct: 1566.\n",
      "accuracy:  29.86%; (non-O)\n",
      "accuracy:  80.39%; precision:  22.37%; recall:  26.47%; FB1:  24.24\n",
      "              LOC: precision:  78.13%; recall:  40.22%; FB1:  53.10  942\n",
      "             MISC: precision:  18.06%; recall:   7.33%; FB1:  10.43  371\n",
      "              ORG: precision:  29.21%; recall:  16.03%; FB1:  20.70  736\n",
      "              PER: precision:  11.06%; recall:  29.91%; FB1:  16.15  4953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main_predict('ner.dev', 'results/model.iter8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3684/3684 [02:52<00:00, 21.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46666 tokens with 5616 phrases; found: 7149 phrases; correct: 1301.\n",
      "accuracy:  26.80%; (non-O)\n",
      "accuracy:  77.67%; precision:  18.20%; recall:  23.17%; FB1:  20.38\n",
      "              LOC: precision:  76.96%; recall:  41.30%; FB1:  53.75  894\n",
      "             MISC: precision:  12.89%; recall:   5.85%; FB1:   8.05  318\n",
      "              ORG: precision:  26.28%; recall:  10.63%; FB1:  15.13  666\n",
      "              PER: precision:   7.53%; recall:  24.78%; FB1:  11.55  5271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main_predict('ner.test', 'results/model.iter8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['<START>', 'SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.', '<STOP>'], 'pos': ['<START>', 'NN', ':', 'NNP', 'VB', 'NNP', 'NNP', ',', 'NNP', 'IN', 'DT', 'NN', '.', '<STOP>'], 'NP_chunk': ['<START>', 'I-NP', 'O', 'I-NP', 'I-VP', 'I-NP', 'I-NP', 'O', 'I-NP', 'I-PP', 'I-NP', 'I-NP', 'O', '<STOP>'], 'gold_tags': ['<START>', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', '<STOP>']}\n"
     ]
    }
   ],
   "source": [
    "data = read_data('ner.test')[:4]\n",
    "parameters = FeatureVector({})\n",
    "parameters.read_from_file('results/model.iter8')\n",
    "\n",
    "tagset = ['B-PER', 'B-LOC', 'B-ORG', 'B-MISC', 'I-PER', 'I-LOC', 'I-ORG', 'I-MISC', 'O']\n",
    "\n",
    "feature_names = ['tag', 'prev_tag', 'current_word', 'curr_pos_tag']\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 19.83it/s]\n"
     ]
    }
   ],
   "source": [
    "all_gold_tags = [ ]\n",
    "all_predicted_tags = [ ]\n",
    "for inputs in tqdm(data):\n",
    "    all_gold_tags.append(inputs['gold_tags'][1:-1]) \n",
    "    input_len = len(inputs['tokens'])\n",
    "    all_predicted_tags.append(predict(inputs, input_len, parameters, feature_names, tagset)[1:-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', 'AL-AIN', ',', 'United', 'Arab', 'Emirates', '1996-12-06', '<STOP>']\n"
     ]
    }
   ],
   "source": [
    "print(data[display_id]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-LOC', 'O', 'I-LOC', 'I-LOC', 'I-LOC', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(all_gold_tags[display_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'I-LOC', 'O', 'I-MISC', 'B-PER']\n"
     ]
    }
   ],
   "source": [
    "print(all_predicted_tags[display_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gHEtAu_LfmPG"
   },
   "outputs": [],
   "source": [
    "!cat \"results/model.iter9\" | awk '{print $2, $1}' | sort -gr > \"results/model.4features.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bS4d4Acfp1S"
   },
   "source": [
    "The file `model.sorted.txt` will be viewable in your Google Drive folder."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP-202_A1_Code.ipynb",
   "provenance": [
    {
     "file_id": "186hCS3cdEtl0vpkgCXHYgLUu-BkFZZ9T",
     "timestamp": 1583157228957
    }
   ]
  },
  "kernelspec": {
   "display_name": "203A3E1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
